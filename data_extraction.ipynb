{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauravshetty98/Gaurav-GIS-Repo/blob/main/data_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Extraction and Cleaning\n",
        "\n",
        "This notebook represents the steps to download the zip file from GitHub repository and extract the data from its content and create a single dataset.\n",
        "\n",
        "Dataset :  https://www.samhsa.gov/data/report/2021-nsduh-state-specific-tables\n",
        "\n",
        "The dataset contains state-wise PDFs with drug abuse and mental health information of the respective state.\n",
        "\n",
        "We start of by installing `tabula` library. This library is used to extract the tables from the PDFs.\n"
      ],
      "metadata": {
        "id": "N0uDqhGy89XW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tabula-py"
      ],
      "metadata": {
        "id": "DtzQaJNsnbfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the library is installed, we move on to import all the required libraries. We make use of google drive to store the PDFs present in the dataset.\n",
        "\n",
        "**This notebook requires your google drive mounting to store dataset files. You can delete those files after running the notebook**\n",
        "\n"
      ],
      "metadata": {
        "id": "AdRWSGKy-V4u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inMGLPGG_0pB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os import path\n",
        "import tabula\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from geopy.geocoders import Nominatim\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content//gdrive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating folders for storing all the data\n",
        "\n",
        "`2021NSDUHsaeSpecificStatesTabs122022` is the name of the new folder. This folder will contain all the state wise PDFs.\n",
        "A `tables` folder is also present, which will contain all the tables extracted from each state PDF.\n",
        "\n"
      ],
      "metadata": {
        "id": "T6wxZ86J_Se8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = \"//content//gdrive//My Drive//2021NSDUHsaeSpecificStatesTabs122022\"\n",
        "folder_name = \"tables\""
      ],
      "metadata": {
        "id": "MPr05FfuBYP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a new folder if it is not present"
      ],
      "metadata": {
        "id": "uQO1CHP6AuAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(dir) == False:\n",
        "    os.mkdir(dir)\n",
        "    print(\"done\")\n",
        "if os.path.exists(dir+\"//\"+folder_name) == False:\n",
        "    os.mkdir(dir+\"//\"+folder_name)\n",
        "    print(\"done\")\n",
        "\n",
        "!ls \"//content//gdrive//My Drive//2021NSDUHsaeSpecificStatesTabs122022\""
      ],
      "metadata": {
        "id": "5gcfXPQ0C9bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the zip file from GitHub and extracting it into the drive folder we created"
      ],
      "metadata": {
        "id": "eVbW17dQBBs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://github.com/gauravshetty98/Gaurav-GIS-Repo/raw/main/2021NSDUHsaeSpecificStatesTabs122022.zip\"\n",
        "!unzip /content/2021NSDUHsaeSpecificStatesTabs122022.zip -d \"//content//gdrive//My Drive//2021NSDUHsaeSpecificStatesTabs122022\""
      ],
      "metadata": {
        "id": "wvVJlMZ65wZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Concatenating all files and creating a single DataFrame\n",
        "\n",
        "We iterate through all PDF files present in the new directory and extract the tables present in the first two pages using `tabula.read_pdf()`\n",
        "\n",
        "We then concatenate the 2 extracted tables into a single dataframe and add a new column `states` to the dataframe.\n",
        "\n",
        "Finally, we convert the dataframe into csv and store it in the `tables` folder in google drive"
      ],
      "metadata": {
        "id": "25Xe8ePwCTVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ext = ('.pdf','.PDF')\n",
        "final_table = pd.DataFrame()\n",
        "for files in os.listdir(dir):\n",
        "    if files.endswith(ext):\n",
        "        print(files)\n",
        "        tables = tabula.read_pdf(dir + \"//\" + files,pages=[1,2]) #address of pdf file\n",
        "        filename = files.replace(\"NSDUHsae\",\"\").replace(\".pdf\",\"\").replace(\"2021\",\"\")\n",
        "        df1 = pd.DataFrame(tables[0])\n",
        "        df2 = pd.DataFrame(tables[1])\n",
        "        final_table = pd.concat([df1,df2])\n",
        "        final_table['states'] = filename\n",
        "        #print(final_table)\n",
        "        final_table.to_csv(os.path.join(dir + \"//\" + folder_name, filename+\".csv\"), index=False)\n",
        "        print(filename)\n",
        "    else:\n",
        "        print(\"Else: \", files)\n",
        "#final_table.to_csv(os.path.join(folder_name, \"FinalTable.csv\"), index=False)"
      ],
      "metadata": {
        "id": "XqxoTTrxFVMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output shows all the CSVs extracted and stored state wise in the tables folder"
      ],
      "metadata": {
        "id": "Rtl2qIVbDh2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"//content//gdrive//My Drive//2021NSDUHsaeSpecificStatesTabs122022/tables\""
      ],
      "metadata": {
        "id": "lRxOXHEnowiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the CSVs are concatenated into a single dataframe."
      ],
      "metadata": {
        "id": "qOvsea8YDg2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_dataset = pd.DataFrame()\n",
        "csv_path = dir + \"//\" + folder_name\n",
        "for tables in os.listdir(csv_path):\n",
        "  df3 = pd.read_csv(csv_path + \"//\" + tables)\n",
        "  final_dataset = pd.concat([final_dataset,df3],ignore_index = True)"
      ],
      "metadata": {
        "id": "0p6W5gMco9RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------\n",
        "## Data Cleaning\n",
        "\n",
        "Here we start with the data cleaning. We first replace all the `NaN` values with 0."
      ],
      "metadata": {
        "id": "Mod4cfH1Yd_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_dataset.head(10))"
      ],
      "metadata": {
        "id": "B2MTYR0PETPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropping Empty Rows\n",
        "\n",
        "The dataset contains some rows which are empty and dont contain any necessary information. Here we search for those rows and drop them from the dataset\n",
        "\n",
        "Count of empty rows in the dataset = 392"
      ],
      "metadata": {
        "id": "PuyO-6YAYygC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zmq import NULL\n",
        "count = 0\n",
        "for i in range(0,final_dataset.shape[0]):\n",
        "  if list(final_dataset.iloc[i,1:6]) == list([np.NaN, np.NaN, np.NaN, np.NaN, np.NaN]):\n",
        "    final_dataset.iloc[i+1,0] = str(final_dataset.iloc[i,0]) + \" \" + str(final_dataset.iloc[i+1,0])\n",
        "    count = count + 1\n",
        "\n",
        "print(\"Number of empty rows: \",count)\n",
        "print(\"Acutal data we need: \", final_dataset.shape[0] - count)"
      ],
      "metadata": {
        "id": "Hl469ou2OxAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We make use of the `dropna()` function to drop all the empty rows. You can see in the resulting dataframe there are no empty rows.\n",
        "\n",
        "Final shape of the dataset = 2016 * 7"
      ],
      "metadata": {
        "id": "8qLUbRI7IJV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_dataset = final_dataset.dropna()\n",
        "final_dataset = final_dataset.reset_index()\n",
        "print(final_dataset.head(10))"
      ],
      "metadata": {
        "id": "v8O9w1IjZEym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Replacing more Null values\n",
        "\n",
        "The dataset still contains some null values. During the extraction process the null values in the PDF were converted to string `\"--\"`. Also there are some `*` in the dataset which represent data with low precision or no estimates. We search for these strings/values and replace it with zero as they are of no use in our dataset."
      ],
      "metadata": {
        "id": "OBngXagoZGfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Before: \", final_dataset.iloc[260,:])\n",
        "final_dataset = final_dataset.replace(\"--\",0)\n",
        "final_dataset = final_dataset.replace(\"*\",0)\n",
        "print(\"After: \", final_dataset.iloc[260,:])"
      ],
      "metadata": {
        "id": "v03e70k0SREZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------\n",
        "### Storing the dataset into a CSV file"
      ],
      "metadata": {
        "id": "NA8I1K1MZqgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_dataset.to_csv(os.path.join(dir + \"//\" + folder_name, \"state_dataset.csv\"), index=False)"
      ],
      "metadata": {
        "id": "T2nop3zEYEkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "The Cleaning of NSDUH dataset is done. This dataset will be further used for the plotting problem sets.\n",
        "\n",
        "This notebook will be updated and new sections will be added if any new dataset is being imported and it requires some cleaning"
      ],
      "metadata": {
        "id": "q4ZdXnsJJZRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding total count to rehab centre dataset"
      ],
      "metadata": {
        "id": "enQV7QWTzNh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -q -O rehabData.csv https://github.com/gauravshetty98/Gaurav-GIS-Repo/raw/main/NSSATS_PUF_2020_CSV.csv"
      ],
      "metadata": {
        "id": "a2Et-JeHrRrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rehab_df = pd.read_csv('rehabData.csv')\n",
        "rehab_df.head()"
      ],
      "metadata": {
        "id": "0RPYqMlRsQOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 1\n",
        "count_list = []\n",
        "for i in range(rehab_df.shape[0]-1):\n",
        "  if rehab_df.iloc[i,1] == rehab_df.iloc[i+1, 1]:\n",
        "    count = count + 1\n",
        "  else:\n",
        "    for y in range(count):\n",
        "      count_list.append(count)\n",
        "    #rehab_df[i,262] = count\n",
        "    #print(rehab_df.iloc[i,1], rehab_df.iloc[i,rehab_df.shape[1]-1])\n",
        "    count = 1\n",
        "for y in range(count):\n",
        "  count_list.append(count)\n",
        "rehab_df['Total Count'] = count_list\n",
        "\n",
        "print(len(count_list))\n",
        "print(rehab_df.shape)\n",
        "rehab_df['STATE'].unique()"
      ],
      "metadata": {
        "id": "3H8mbkOdsoC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rehab_df.to_csv('rehab_dataset.csv')"
      ],
      "metadata": {
        "id": "07y4jCFQtM2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -q -O mental_rehab.csv https://github.com/gauravshetty98/Gaurav-GIS-Repo/raw/main/nmhss-puf-2020-csv.csv"
      ],
      "metadata": {
        "id": "c2tzvaBd-Gmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mental_df = pd.read_csv('mental_rehab.csv')\n",
        "mental_df.head()"
      ],
      "metadata": {
        "id": "Xr4nW5kU-z4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 1\n",
        "count_list = []\n",
        "for i in range(mental_df.shape[0]-1):\n",
        "  if mental_df.iloc[i,1] == mental_df.iloc[i+1, 1]:\n",
        "    count = count + 1\n",
        "  else:\n",
        "    for y in range(count):\n",
        "      count_list.append(count)\n",
        "    #rehab_df[i,262] = count\n",
        "    #print(rehab_df.iloc[i,1], rehab_df.iloc[i,rehab_df.shape[1]-1])\n",
        "    count = 1\n",
        "for y in range(count):\n",
        "  count_list.append(count)\n",
        "mental_df['Total Count'] = count_list\n",
        "\n",
        "print(len(count_list))\n",
        "print(mental_df.shape)\n",
        "print(mental_df['LST'].unique())\n",
        "mental_df['Total Count'].unique()"
      ],
      "metadata": {
        "id": "Hvu7lDOH_Mx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mental_df.to_csv('mental_health_treatment.csv')"
      ],
      "metadata": {
        "id": "2ykP5io-_qhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O rehab_dir.xlsx https://github.com/gauravshetty98/Gaurav-GIS-Repo/raw/main/national-directory-su-facilities-2023.xlsx"
      ],
      "metadata": {
        "id": "OnBRzgtUBZTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rehab_dir = pd.read_excel('rehab_dir.xlsx', sheet_name = 0)\n",
        "print(rehab_dir.head())\n",
        "print(rehab_dir.shape)"
      ],
      "metadata": {
        "id": "_j_N1WpPzeC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loc = Nominatim(user_agent=\"Geopy Library\")\n",
        "\n",
        "getLoc = loc.geocode(\"İzmir\")\n",
        "\n",
        "print(\"Latitude = \", getLoc.latitude, \"\\n\")\n",
        "print(\"Longitude = \", getLoc.longitude)"
      ],
      "metadata": {
        "id": "A9P8bTv4zmBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lat_list = []\n",
        "long_list = []\n",
        "\n",
        "for i in range(0,rehab_dir.shape[0]):\n",
        "  try:\n",
        "    if len(str(rehab_dir.iloc[i,6])) == 4:\n",
        "      getLoc = loc.geocode(\"0\"+str(rehab_dir.iloc[i,6])+\" , united states\")\n",
        "      lat_list.append(getLoc.latitude)\n",
        "      long_list.append(getLoc.longitude)\n",
        "    else:\n",
        "      getLoc = loc.geocode(str(rehab_dir.iloc[i,6])+\" , united states\")\n",
        "      lat_list.append(getLoc.latitude)\n",
        "      long_list.append(getLoc.longitude)\n",
        "  except:\n",
        "    lat_list.append(0)\n",
        "    long_list.append(0)\n",
        "    print(\"not found: \", i)\n",
        "\n",
        "print(len(lat_list))\n",
        "print(len(long_list))"
      ],
      "metadata": {
        "id": "9x0A28_T0GCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rehab_dir['lat'] = lat_list\n",
        "rehab_dir['long'] = long_list\n",
        "rehab_dir.to_csv('rehab_dir_24oct.csv')"
      ],
      "metadata": {
        "id": "NN4W4OTN1D1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G1WgTxBfXW25"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}